{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will be creating a relatively simple `ConvNet classifier` that is capable of classifying between <i>Hot Dogs</i> and <i>Not Hot Dogs</i>.\n",
    "\n",
    "A `Convolutional Neural Network` is a type of neural network that is used in <u>Computer Vision</u> and <u>Natural Language Processing</u> tasks quite often due to the fact that it can learn to extract relevant features from the input data.\n",
    "\n",
    "it can do two things really well if it is trained properly:\n",
    "- Generate predictions for new input samples (output layer, which outputs the predictions of the model. This can either be a <u>binary prediction,</u> which is the case for our hot-dog classifier, or a <u>multiclass/multilabel</u> prediction.)\n",
    "- Extract relevant features from the input data to generate those predictions.\n",
    "\n",
    "\n",
    "It is not surprising to find (1), which is our case for this classifier, with this class of machine learning models, or with any machine learning model, because it is the essence of the `supervised machine learning` process: training a model with some data in order to make it capable of generating new <u>predictions</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can build the model, it's important that you ensure that your development environment is ready..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow numpy opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TensorFlow` is an end-to-end open source platform for machine learning.\n",
    "\n",
    "`keras` provides essential abstractions and building blocks for developing and shipping machine learning solutions with high iteration velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from keras.models import Sequential, load_model\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the model configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll be using 25 x 25 pixel images that are grayscale (hence the <i>1</i> in the `input_shape`), use a `batch size` of <i>10</i> (our data set will be relatively small), <i>25</i> iterations, <i>2</i> classes (not hot dog = 0 / hot dog = 1), and <i>20%</i> of our data will be used for <u>testing purposes</u>. We make the training process <u>verbose</u>, meaning that all results will be printed on screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "img_width, img_height = 25, 25\n",
    "input_shape = (img_width, img_height, 1)\n",
    "batch_size = 10\n",
    "no_epochs = 25\n",
    "no_classes = 2\n",
    "validation_split = 0.2\n",
    "verbosity = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preprocessing of the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset is available at [Kaggle](https://www.kaggle.com/datasets/thedatasith/hotdog-nothotdog/data)\n",
    "\n",
    "After downloading, unzip the data, and make sure u follow the following <u>directory structure</u> with respect to this <u>naming conventions</u>:\n",
    "\n",
    "``` bash\n",
    "├── assets \n",
    "    ├── test\n",
    "        ├── hot_dog\n",
    "        ├── not_hot_dog   \n",
    "    ├── train\n",
    "        ├── hot_dog\n",
    "        ├── not_hot_dog\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the following procedure does a couple of things:\n",
    "- It allows us to specify the `data_type` and the `class_name`. By default, it attempts to load <i>hot_dog</i> images from the <i>train</i> folder.\n",
    "- It loads the image using imread, as a `grayscale` image - by means of the <i>0</i> specification. We don't want colors of images to interfere with the prediction, as it's all about shape. This is especially important in the case of small datasets, which can be biased. That's why we load the images as `grayscale` ones.\n",
    "We resize the images to `25 x 25` pixels, in line with the model configuration specified above. Resizing is necessary for two reasons.\n",
    "    - Firstly, images can be really large sometimes, and this can hamper learning. It's usually best to train your models with images relatively small in size. \n",
    "    - Secondly, your model will accept inputs only when they have the shape of the input specified in the Input layer. That's why all images must be using the same number of color channels (that is, either RGB or grayscale, but not both) and be of the same size.\n",
    "- We append the resized image to the <u>list of instances</u>, and the corresponding class number to the <u>list of classes</u>.\n",
    "- We return a `tuple` with the <u>instances and classes.</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "def load_data(data_type='train', class_name='hot_dog'):\n",
    "  instances = []\n",
    "  classes = []\n",
    "  for filepath in os.listdir(f'assets/{data_type}/{class_name}'):\n",
    "    resized_image = cv2.imread(f'assets/{data_type}/{class_name}/{format(filepath)}', 0)\n",
    "    resized_image = cv2.resize(resized_image, (img_width, img_height))\n",
    "    instances.append(resized_image)\n",
    "    classes.append(0 if class_name == 'not_hot_dog' else 1)\n",
    "  return (instances, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model skeleton\n",
    "we can move on and create a function that creates the model skeleton. Such a skeleton is essentially the representation of the model building blocks - i.e., the architecture. The model itself is not yet alive, and will be instantiated after specifying the skeleton.\n",
    "\n",
    "The steps performed are simple: a model is created with the `Sequential API`, [three convolutional layers](https://machinecurve.com/index.php/2018/12/07/convolutional-neural-networks-and-their-components-for-computer-vision) are stacked on top of each other (note the increasing number of feature maps with increasing abstractness; we benefit most from learning the abstract representations), a `Flatten` operation which allows the output feature maps to be input by the `Dense` layers, which finally generate a [multiclass probability distribution using Softmax](https://machinecurve.com/index.php/2020/01/08/how-does-the-softmax-activation-function-work).\n",
    "\n",
    "It finally returns the `model` after creating the skeleton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model creation\n",
    "def create_model():\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(4, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "  model.add(Conv2D(8, kernel_size=(3, 3), activation='relu'))\n",
    "  model.add(Conv2D(12, kernel_size=(3, 3), activation='relu'))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(256, activation='relu'))\n",
    "  model.add(Dense(no_classes, activation='softmax'))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we specify things like the `loss function` (we use sparse categorical crossentropy loss because our targets, our y values, are <i>integers</i> rather than one-hot encoded vectors).\n",
    "We also specify the optimize, which can be `gradient descent-based` or `Adaptive`, like `Adam`.\n",
    "In addition, we specify <u>additional metrics</u>. \n",
    "\n",
    "We then return the model again to be used by the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model compilation\n",
    "def compile_model(model):\n",
    "  model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the training process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we accept the <u>model</u> as well as the <u>features</u> and <u>corresponding targets</u> from the training set. Using `configuration options` specified in the model configuration (such as batch size, number of epochs, and verbosity) we start the training process. We do so by calling `model.fit`, which essentially fits the data to the model and attempts to find the `global loss minimum`. Once training has finished, which in our case happens after <i>25 iterations</i> (or epochs), the trained model is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "def train_model(model, X_train, y_train):\n",
    "  model.fit(X_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=no_epochs,\n",
    "            verbose=verbosity,\n",
    "            shuffle=True,\n",
    "            validation_split=validation_split)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating evaluation metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function accepts the <u>trained model</u> as well as the <u>features</u> and <u>targets</u> of your testing dataset. It evaluates the model with those samples and prints test `loss` and `accuracy`. For convenience reasons, this function also returns the trained (and now tested) model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model testing\n",
    "def test_model(model, X_test, y_test):\n",
    "  score = model.evaluate(X_test, y_test, verbose=0)\n",
    "  print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting the building blocks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now time to connect the dots, and specifically do those two things:\n",
    "\n",
    "- Load and merge training and testing data\n",
    "- Constructing the model\n",
    "\n",
    "For both data sets, we use load_data to retrieve our `hot-dog` / `not-hot-dog` data, and eventually merge the two sub datasets each time and create a `np.array` with all the training and testing data, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLICKING EVERYTHING TOGETHER\n",
    "# Load and merge training data\n",
    "X_train_nh, y_train_nh = load_data(data_type='train', class_name='not_hot_dog')\n",
    "X_train_h, y_train_h = load_data(data_type='train', class_name='hot_dog')\n",
    "X_train = np.array(X_train_nh + X_train_h)\n",
    "X_train = X_train.reshape((X_train.shape[0], img_width, img_height, 1))\n",
    "y_train = np.array(y_train_nh + y_train_h)\n",
    "\n",
    "# Load and merge testing data\n",
    "X_test_nh, y_test_nh = load_data(data_type='test', class_name='not_hot_dog')\n",
    "X_test_h, y_test_h = load_data(data_type='test', class_name='hot_dog')\n",
    "X_test = np.array(X_test_nh + X_test_h)\n",
    "X_test = X_test.reshape((X_test.shape[0], img_width, img_height, 1))\n",
    "y_test = np.array(y_test_nh + y_test_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, constructing the model is essentially connecting the functions we defined above:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "  5/340 [..............................] - ETA: 13s - loss: 27.0383 - accuracy: 0.5200"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 06:27:43.056784: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 26303904 exceeds 10% of free system memory.\n",
      "2023-12-13 06:27:43.058556: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 26303904 exceeds 10% of free system memory.\n",
      "2023-12-13 06:27:43.070205: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 25401600 exceeds 10% of free system memory.\n",
      "2023-12-13 06:27:43.071108: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 25401600 exceeds 10% of free system memory.\n",
      "2023-12-13 06:27:43.079569: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 21672072 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - 16s 42ms/step - loss: 1.1441 - accuracy: 0.6015 - val_loss: 0.9559 - val_accuracy: 0.0824\n",
      "Epoch 2/25\n",
      "340/340 [==============================] - 13s 38ms/step - loss: 0.5314 - accuracy: 0.7271 - val_loss: 1.1153 - val_accuracy: 0.2662\n",
      "Epoch 3/25\n",
      "340/340 [==============================] - 13s 39ms/step - loss: 0.3208 - accuracy: 0.8786 - val_loss: 1.5773 - val_accuracy: 0.3428\n",
      "Epoch 4/25\n",
      "340/340 [==============================] - 12s 36ms/step - loss: 0.1167 - accuracy: 0.9629 - val_loss: 1.5671 - val_accuracy: 0.4122\n",
      "Epoch 5/25\n",
      "340/340 [==============================] - 13s 37ms/step - loss: 0.0321 - accuracy: 0.9926 - val_loss: 3.0989 - val_accuracy: 0.3015\n",
      "Epoch 6/25\n",
      "340/340 [==============================] - 13s 39ms/step - loss: 0.0270 - accuracy: 0.9929 - val_loss: 1.9870 - val_accuracy: 0.4158\n",
      "Epoch 7/25\n",
      "340/340 [==============================] - 11s 32ms/step - loss: 0.0485 - accuracy: 0.9838 - val_loss: 3.2032 - val_accuracy: 0.3004\n",
      "Epoch 8/25\n",
      "340/340 [==============================] - 14s 40ms/step - loss: 0.0074 - accuracy: 0.9988 - val_loss: 2.2756 - val_accuracy: 0.4700\n",
      "Epoch 9/25\n",
      "340/340 [==============================] - 13s 37ms/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 3.9687 - val_accuracy: 0.3451\n",
      "Epoch 10/25\n",
      "340/340 [==============================] - 17s 51ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.3373 - val_accuracy: 0.3322\n",
      "Epoch 11/25\n",
      "340/340 [==============================] - 17s 51ms/step - loss: 1.9328e-04 - accuracy: 1.0000 - val_loss: 4.6050 - val_accuracy: 0.3310\n",
      "Epoch 12/25\n",
      "340/340 [==============================] - 23s 66ms/step - loss: 9.0449e-05 - accuracy: 1.0000 - val_loss: 4.8918 - val_accuracy: 0.3239\n",
      "Epoch 13/25\n",
      "340/340 [==============================] - 23s 69ms/step - loss: 6.1604e-05 - accuracy: 1.0000 - val_loss: 5.0963 - val_accuracy: 0.3239\n",
      "Epoch 14/25\n",
      "340/340 [==============================] - 19s 55ms/step - loss: 4.4931e-05 - accuracy: 1.0000 - val_loss: 5.2597 - val_accuracy: 0.3239\n",
      "Epoch 15/25\n",
      "340/340 [==============================] - 17s 49ms/step - loss: 3.4105e-05 - accuracy: 1.0000 - val_loss: 5.3727 - val_accuracy: 0.3333\n",
      "Epoch 16/25\n",
      "340/340 [==============================] - 26s 77ms/step - loss: 2.6524e-05 - accuracy: 1.0000 - val_loss: 5.5279 - val_accuracy: 0.3298\n",
      "Epoch 17/25\n",
      "340/340 [==============================] - 26s 78ms/step - loss: 2.0983e-05 - accuracy: 1.0000 - val_loss: 5.6299 - val_accuracy: 0.3286\n",
      "Epoch 18/25\n",
      "340/340 [==============================] - 28s 81ms/step - loss: 1.6909e-05 - accuracy: 1.0000 - val_loss: 5.7483 - val_accuracy: 0.3274\n",
      "Epoch 19/25\n",
      "340/340 [==============================] - 16s 48ms/step - loss: 1.3572e-05 - accuracy: 1.0000 - val_loss: 5.8434 - val_accuracy: 0.3298\n",
      "Epoch 20/25\n",
      "340/340 [==============================] - 12s 36ms/step - loss: 1.0949e-05 - accuracy: 1.0000 - val_loss: 5.9696 - val_accuracy: 0.3274\n",
      "Epoch 21/25\n",
      "340/340 [==============================] - 15s 45ms/step - loss: 8.9045e-06 - accuracy: 1.0000 - val_loss: 6.0760 - val_accuracy: 0.3274\n",
      "Epoch 22/25\n",
      "340/340 [==============================] - 19s 57ms/step - loss: 7.1673e-06 - accuracy: 1.0000 - val_loss: 6.1746 - val_accuracy: 0.3274\n",
      "Epoch 23/25\n",
      "340/340 [==============================] - 22s 63ms/step - loss: 5.8146e-06 - accuracy: 1.0000 - val_loss: 6.2882 - val_accuracy: 0.3251\n",
      "Epoch 24/25\n",
      "340/340 [==============================] - 24s 70ms/step - loss: 4.7392e-06 - accuracy: 1.0000 - val_loss: 6.3708 - val_accuracy: 0.3239\n",
      "Epoch 25/25\n",
      "340/340 [==============================] - 22s 66ms/step - loss: 3.8734e-06 - accuracy: 1.0000 - val_loss: 6.4601 - val_accuracy: 0.3251\n",
      "Test loss: 4.055922508239746 / Test accuracy: 0.5274999737739563\n"
     ]
    }
   ],
   "source": [
    "# Create and train the model\n",
    "model = create_model()\n",
    "model = compile_model(model)\n",
    "model = train_model(model, X_train, y_train)\n",
    "model = test_model(model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
